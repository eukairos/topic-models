{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f663dd6f-5bac-4a2d-91d0-59e5346c416e",
   "metadata": {},
   "source": [
    "## GridSearch For Best UMAP and HDBSCAN hyperparameters for PubMed abstracts\n",
    "This notebook documents partially GridSearch for the optimal UMAP and HDBSCAN parameters for our corpus of PubMed abstracts in our topic modelling exploration. \n",
    "\n",
    "The hyperparameters tested were: n_components and n_neighbors for UMAP, and min_cluster_size and min_samples for HDBSCAN.  \n",
    "*n_components* is essentially the dimension size after dimensionality reduction. Although the default is 5, I swept values between 5-50. Somewhere around 20 is the sweet spot.\n",
    "*n_neighbors* control the balance between local and global structure. Small values fragment larger topics, and large values produce more cohesive, well-separated clusters. The UMAP documentation suggests increasing up to 30 when using UMAP as a preprocessing step for density-based clustering (https://umap-learn.readthedocs.io/en/latest/clustering.html)  \n",
    "*min_cluster_size* and *min_samples* - based on the BERTopic documentation (https://maartengr.github.io/BERTopic/getting_started/parameter%20tuning/parametertuning.html#metric) min_samples should be set significantly below min_cluster_size to reduce noise.\n",
    "\n",
    "To my knowledge, there are no previous studies on hyperparameter tuning for BERTopic for topic modelling on PubMed abstracts, so we have little to go by for starting points, which is why I deployed a large range for the hyperparameter search.\n",
    "\n",
    "The metric chosen was Density-Based Clustering Validation (DBCV), which is an intrinsic evaluation metric for clustering. DBCV compares intra-cluster density against inter-cluster density. Scores range from 1 (dense, well-separated) to -1 (points are closer to neighboring clusters than their own). A value of zero is essentially random, without any structure. DBCV is particularly suitable for UMAP and HDBSCAN as it evaluates cluster quality directly in the reduced embedding space without requiring ground-truth labels.  \n",
    "\n",
    "The search started with a wide range for UMAP, in steps of 5, then progressively narrowing down both range and steps. For each iteration, a range of 2 around the best hyperparameter value was tested. For example, if DBCV was highest at n_components = 21 when tested for 10,15,20,25,30 then it was retested for n_components 19,20,21,22,23.  \n",
    "\n",
    "UMAP was first tested, and a range around the optimal n_components and n_neighbor combination used to test against HDBSCAN hyperparameters.\n",
    "The cells below show the last iterations performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "badba649-4df0-49cc-acce-0af5ac902e40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that GPU is available for use\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a71ba2d-865f-4ef8-9cd7-a9f80a700884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the embedding model\n",
    "from sentence_transformers import SentenceTransformer\n",
    "embed_model = SentenceTransformer('neuml/pubmedbert-base-embeddings', device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cc240e2-4605-478b-9ecd-b2943941b470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "import pandas as pd\n",
    "data = pd.read_csv('pubmed_abstracts.csv')\n",
    "to_drop = ['Title','pmid','meshMajor', 'meshid', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'L', 'M', 'N', 'Z']\n",
    "data = data.drop(to_drop, axis=1)\n",
    "data = data.sample(n=5000, random_state=42)\n",
    "data = data.reset_index(drop=True)\n",
    "data['abstractText'] = data['abstractText'].str.lower()\n",
    "\n",
    "# A bit of cleaning to remove numbers\n",
    "import re \n",
    "def remove_numbers(series):\n",
    "    def rem_no(text):\n",
    "        pattern = r'\\b\\d+(\\.\\d{1,2})?\\b'\n",
    "        cleaned_text = re.sub(pattern, '', text)\n",
    "        cleaned_text = cleaned_text.strip()\n",
    "        return cleaned_text\n",
    "    return series.apply(rem_no)\n",
    "\n",
    "# Extract out just the abstracts to a list - BERTopic works with lists\n",
    "data['no_numbers'] = remove_numbers(data['abstractText'])\n",
    "abstracts = data['no_numbers'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b16d4cde-c4a7-4150-9cc9-ffa399f6d7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the embeddings\n",
    "embeddings = embed_model.encode(abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69139f0a-af8e-45a3-9ee5-bc3bb439641e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UMAP n_components sweep:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " n_components = 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UMAP n_components sweep:  25%|██▌       | 1/4 [00:05<00:17,  5.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Clusters : 130\n",
      " Noise    : 169133.8%\n",
      " DBCS     : 0.2099\n",
      "\n",
      " n_components = 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UMAP n_components sweep:  50%|█████     | 2/4 [00:11<00:11,  5.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Clusters : 121\n",
      " Noise    : 154230.8%\n",
      " DBCS     : 0.1857\n",
      "\n",
      " n_components = 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UMAP n_components sweep:  75%|███████▌  | 3/4 [00:16<00:05,  5.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Clusters : 126\n",
      " Noise    : 161932.4%\n",
      " DBCS     : 0.1192\n",
      "\n",
      " n_components = 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UMAP n_components sweep: 100%|██████████| 4/4 [00:22<00:00,  5.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Clusters : 129\n",
      " Noise    : 173934.8%\n",
      " DBCS     : 0.1474\n",
      "\n",
      "  SUMMARY: ---------------------------\n",
      "\n",
      "Best n_components by DBCV: 22  (DBCV = 0.2099)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Do the GridSearch for UMAP first\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from umap import UMAP\n",
    "from tqdm import tqdm\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "dimensions = [20, 21, 22, 23, 24, 25]\n",
    "results = []\n",
    "\n",
    "for dimension in tqdm(dimensions, desc='UMAP n_components sweep'):\n",
    "    print(f'\\n n_components = {dimension}')\n",
    "\n",
    "    # UMAP\n",
    "    reducer = UMAP(\n",
    "        n_components = dimension,\n",
    "        n_neighbors = 15,\n",
    "        min_dist = 0.0,\n",
    "        metric = 'cosine',\n",
    "        random_state = 42,\n",
    "        verbose = False\n",
    "    )\n",
    "    reduced = reducer.fit_transform(embeddings)\n",
    "\n",
    "    # HDBSCAN\n",
    "    clusterer = HDBSCAN(\n",
    "        min_cluster_size=10,\n",
    "        min_samples = 5,\n",
    "        metric = 'euclidean',\n",
    "        cluster_selection_method = 'eom',\n",
    "        gen_min_span_tree = True # required for dbcv (relative validity_)\n",
    "    )\n",
    "    clusterer.fit(reduced)\n",
    "\n",
    "    labels = clusterer.labels_\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_noise = int((labels == -1).sum())\n",
    "    noise_pct = 100 * n_noise / len(labels)\n",
    "    dbcv = clusterer.relative_validity_\n",
    "\n",
    "    print(f' Clusters : {n_clusters}')\n",
    "    print(f' Noise    : {n_noise}{noise_pct:.1f}%')\n",
    "    print(f' DBCS     : {dbcv:.4f}')\n",
    "\n",
    "    results.append({\n",
    "        'n_components' : dimension,\n",
    "        'n_clusters' : n_clusters,\n",
    "        'n_noise' : n_noise,\n",
    "        'noise_pct' : round(noise_pct, 2),\n",
    "        'dbcv' : round(dbcv, 4)\n",
    "    })\n",
    "\n",
    "print('\\n  SUMMARY: ---------------------------')\n",
    "df = pd.DataFrame(results).set_index('n_components')\n",
    "best = df['dbcv'].idxmax()\n",
    "print(f\"\\nBest n_components by DBCV: {best}  (DBCV = {df.loc[best, 'dbcv']})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b956141c-c3d5-4fdf-a385-0fbceb5cfd39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [03:36<00:00,  7.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " n_components  n_neighbors  n_clusters  noise_pct   dbcv\n",
      "           24            5         177      18.20 0.2358\n",
      "           22            6         156      21.74 0.2274\n",
      "           21            6         159      20.46 0.2184\n",
      "           22            5         172      20.22 0.2159\n",
      "           20            6         157      22.16 0.2122\n",
      "           22            7         157      22.84 0.2115\n",
      "           23            6         169      23.26 0.2089\n",
      "           23            7         148      23.70 0.2063\n",
      "           19            7         153      23.96 0.2035\n",
      "           20            7         152      24.66 0.1977\n",
      "           19            8         149      26.32 0.1927\n",
      "           24            6         161      22.56 0.1901\n",
      "           20            8         146      27.38 0.1887\n",
      "           24            7         150      22.66 0.1884\n",
      "           20            5         179      20.86 0.1836\n",
      "           20            9         131      30.32 0.1833\n",
      "           19            5         169      18.62 0.1828\n",
      "           21            7         150      23.46 0.1766\n",
      "           24            9         139      28.54 0.1758\n",
      "           23            8         144      27.42 0.1725\n",
      "           21            8         143      24.52 0.1723\n",
      "           23            5         168      17.48 0.1700\n",
      "           22            8         144      26.48 0.1671\n",
      "           19            6         157      23.38 0.1662\n",
      "           22            9         151      29.02 0.1645\n",
      "           24            8         147      26.36 0.1636\n",
      "           23            9         141      30.36 0.1612\n",
      "           19            9         138      30.10 0.1608\n",
      "           21            5         165      18.32 0.1554\n",
      "           21            9         145      30.12 0.1496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "dimensions = range(19,25)\n",
    "neighbors = range(5,10)\n",
    "results = []\n",
    "\n",
    "for n_components, n_neighbors in tqdm(list(product(dimensions, neighbors))):\n",
    "    reducer = UMAP(\n",
    "        n_components=n_components,\n",
    "        n_neighbors=n_neighbors,\n",
    "        min_dist=0.0,\n",
    "        metric=\"cosine\",\n",
    "        random_state=42,\n",
    "    )\n",
    "    reduced = reducer.fit_transform(embeddings)\n",
    "\n",
    "    clusterer = HDBSCAN(\n",
    "        min_cluster_size=10,\n",
    "        min_samples=5,\n",
    "        gen_min_span_tree=True,\n",
    "    ).fit(reduced)\n",
    "\n",
    "    labels = clusterer.labels_\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    noise_pct  = 100 * (labels == -1).sum() / len(labels)\n",
    "\n",
    "    results.append({\n",
    "        \"n_components\": n_components,\n",
    "        \"n_neighbors\":  n_neighbors,\n",
    "        \"n_clusters\":   n_clusters,\n",
    "        \"noise_pct\":    round(noise_pct, 2),\n",
    "        \"dbcv\":         round(clusterer.relative_validity_, 4),\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "print(df.sort_values(\"dbcv\", ascending=False).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9575e6-f68a-4f61-b610-9289b7e67b03",
   "metadata": {},
   "source": [
    "The best combination of dimensions and neighbors is 20, 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ed3aac9-bc78-46b9-a8b6-b0f7747069a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the best combination for dimensionality reduction\n",
    "\n",
    "reducer = UMAP(\n",
    "        n_components=20,\n",
    "        n_neighbors=10,\n",
    "        min_dist=0.0,\n",
    "        metric=\"cosine\",\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "reduced = reducer.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cbf2ca48-8bb5-4e5a-9be4-9a2025536f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 533/533 [03:46<00:00,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " min_cluster_size  min_samples  n_clusters  noise_pct   dbcv\n",
      "               25            5          61      28.26 0.3080\n",
      "               20           13          62      37.36 0.3072\n",
      "               11            5         117      28.44 0.2995\n",
      "               23            5          66      27.44 0.2993\n",
      "               24            5          63      28.34 0.2926\n",
      "               13            5         100      29.42 0.2879\n",
      "               35            7          36      26.40 0.2838\n",
      "               19           13          66      37.40 0.2814\n",
      "               26            5          56      28.68 0.2803\n",
      "               14            5          96      29.78 0.2797\n",
      "               37            7          32      25.42 0.2783\n",
      "               36            7          32      25.42 0.2783\n",
      "               50           12          27      29.34 0.2775\n",
      "               19            5          77      28.78 0.2775\n",
      "               49           12          27      29.34 0.2775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Do the GridSearch for HDBSCAN\n",
    "\n",
    "MIN_CLUSTER_SIZES   = range(10,51)\n",
    "MIN_SAMPLES_LIST    = range(3,16)\n",
    "\n",
    "results = []\n",
    "\n",
    "for min_cluster_size, min_samples in tqdm(\n",
    "    list(product(MIN_CLUSTER_SIZES, MIN_SAMPLES_LIST))\n",
    "):\n",
    "    # Skip nonsensical combinations\n",
    "    if min_samples > min_cluster_size:\n",
    "        continue\n",
    "\n",
    "    clusterer = HDBSCAN(\n",
    "        min_cluster_size=min_cluster_size,\n",
    "        min_samples=min_samples,\n",
    "        cluster_selection_method='eom',\n",
    "        metric=\"euclidean\",\n",
    "        gen_min_span_tree=True,\n",
    "    ).fit(reduced)\n",
    "\n",
    "    labels = clusterer.labels_\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    noise_pct  = 100 * (labels == -1).sum() / len(labels)\n",
    "\n",
    "    results.append({\n",
    "        \"min_cluster_size\": min_cluster_size,\n",
    "        \"min_samples\":      min_samples,\n",
    "        \"n_clusters\":       n_clusters,\n",
    "        \"noise_pct\":        round(noise_pct, 2),\n",
    "        \"dbcv\":             round(clusterer.relative_validity_, 4),\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "print(df.sort_values(\"dbcv\", ascending=False).head(15).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e501b9a-7d50-4fa8-873e-bb0d2b3fffed",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = HDBSCAN(\n",
    "        min_cluster_size=25,\n",
    "        min_samples=5,\n",
    "        cluster_selection_method='eom',\n",
    "        metric=\"euclidean\",\n",
    "        gen_min_span_tree=True,\n",
    "    ).fit(reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcbf41d-e323-422b-b314-9bde39ef93b8",
   "metadata": {},
   "source": [
    "The best parameters are:  \n",
    "UMAP: n_components = 20, n_neighbors = 10  \n",
    "HDBSCAN: min_cluster_size = 25, min_sample = 10  \n",
    "We'll fit these into our BERTopic pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77349f7b-62ef-4b35-846f-47c6c7049f7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
