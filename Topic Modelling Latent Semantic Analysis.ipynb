{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a789476f-b1b0-421f-8f65-fdb32fd9593a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Our toy corpus\n",
    "docs = [\n",
    "    \"ball game player score team\",\n",
    "    \"cook recipe taste food eat\",\n",
    "    \"code computer software program data\",\n",
    "    \"team player win ball coach\",\n",
    "    \"eat delicious cook meal taste\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21ae0fc-7fdd-4fcc-8652-b43a32bbab8a",
   "metadata": {},
   "source": [
    "#### Step 1: Construct a Document-Term Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea9ee08d-cc8c-4a90-ab9d-2cdfa8204849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['ball', 'coach', 'code', 'computer', 'cook', 'data', 'delicious', 'eat', 'food', 'game', 'meal', 'player', 'program', 'recipe', 'score', 'software', 'taste', 'team', 'win']\n",
      "\n",
      "Document-Term Matrix:\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# extract unique terms (ie our vocabulary)\n",
    "vocab = sorted(set(word for doc in docs for word in doc.split()))\n",
    "print('Vocabulary:', vocab)\n",
    "\n",
    "# Build the DTM\n",
    "def build_dtm(docs, vocab):\n",
    "    dtm = np.zeros((len(docs), len(vocab)))\n",
    "    for i, doc in enumerate(docs):\n",
    "        for word in doc.split():\n",
    "            j = vocab.index(word)\n",
    "            dtm[i,j] += 1\n",
    "    return dtm\n",
    "\n",
    "A = build_dtm(docs, vocab)\n",
    "print('\\nDocument-Term Matrix:')\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccc50c2-1b01-42e5-be3c-9fa736a5c128",
   "metadata": {},
   "source": [
    "\n",
    "#### Step 2: Singular Value Decomposition\n",
    "LSA decomposes the matrix as A = U Σ Vᵀ where \n",
    "A(m,n) is our DTM, which decomposes to :  \n",
    "U(m,m) - each row represents a document, and columns (the left singular vectors) represent topics/concepts found in the corpus.   \n",
    "Σ(m,n) - is a diagonal matrix (the singular value matrix) where the diagonals (the singular values of A, typically represented by σ1, σ2, σ3....σm) are, in descending order, values describing the strength of concepts discovered in the corpus,  \n",
    "Vᵀ(n,n) - each column represents a term (remember, this is a transpose) and rows represent topics/concepts found in the corpus. Collectively, the column vectors of Vᵀ are the right singular vectors.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b696c3d8-3dd7-4dd2-ba8c-74a6bc955bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of U (documents × topics): (5, 5)\n",
      "Shape of Σ (singular values): (5,)\n",
      "Shape of Vᵀ (topics × terms): (19, 19)\n",
      "\n",
      "=== SINGULAR VALUES (Σ) ===\n",
      "[2.82842712 2.82842712 2.23606798 1.41421356 1.41421356]\n",
      "\n",
      "Explained variance ratio:\n",
      "  Component 1: σ=2.8284, variance=32.00%\n",
      "  Component 2: σ=2.8284, variance=32.00%\n",
      "  Component 3: σ=2.2361, variance=20.00%\n",
      "  Component 4: σ=1.4142, variance=8.00%\n",
      "  Component 5: σ=1.4142, variance=8.00%\n"
     ]
    }
   ],
   "source": [
    "U, sigma, Vt = np.linalg.svd(A, full_matrices=True)\n",
    "\n",
    "print(\"Shape of U (documents × topics):\", U.shape)\n",
    "print(\"Shape of Σ (singular values):\", sigma.shape)\n",
    "print(\"Shape of Vᵀ (topics × terms):\", Vt.shape)\n",
    "\n",
    "print(\"\\n=== SINGULAR VALUES (Σ) ===\")\n",
    "print(sigma)\n",
    "print(\"\\nExplained variance ratio:\")\n",
    "variance_ratio = (sigma**2) / np.sum(sigma**2)\n",
    "for i, (s, v) in enumerate(zip(sigma, variance_ratio)):\n",
    "    print(f\"  Component {i+1}: σ={s:.4f}, variance={v:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf06e99-1f6f-40dd-890c-88c4fcc5bbd4",
   "metadata": {},
   "source": [
    "The singular value matrix Σ exists in a different 'latent' space between the documents and the vocabulary. Each singular value (denoted by σ1, σ2, σ3....σm) represents a concept (in our case, a topic) and its value indicates how much variance in the data is explained by that concept. We can see that there are 2 main concepts dominating the documents, and a 3rd concept that is more prominent than the remaining 2 concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295686a0-4a6c-40dc-8913-a5c05d1a475b",
   "metadata": {},
   "source": [
    "#### Step 3: Reduce to k Latent Topics\n",
    "This step is optional, but usually done for a large corpus, because we're interested usually in the top-k topics. This reduces computational effort downstream. Here we set k=3 since we think there are 3 main concepts in the corpus. This is an example of the 'elbow' method - the point where the sigma values drop dramatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3faa3f07-928f-419c-baae-7669b304aef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced to 3 latent dimensions\n",
      "Singular values retained: [2.82842712 2.82842712 2.23606798]\n"
     ]
    }
   ],
   "source": [
    "k = 3\n",
    "U_k = U[:, :k]  # document-topic matrix\n",
    "sigma_k = sigma[:k] # top k singular values\n",
    "Vt_k = Vt[:k, :]\n",
    "print(f\"Reduced to {k} latent dimensions\")\n",
    "print(f\"Singular values retained: {sigma_k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47901930-10a5-486b-9a0a-b72d1067a4e3",
   "metadata": {},
   "source": [
    "#### Step 4: Interpret the Discovered Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fca7a2b0-2792-4f67-954e-eec5c08245b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "DISCOVERED LATENT TOPICS\n",
      "==================================================\n",
      "\n",
      "Topic 1:\n",
      " Top Terms: ['taste', 'eat', 'cook', 'delicious', 'meal']\n",
      " Weights:   ['0.500', '0.500', '0.500', '0.250', '0.250']\n",
      "\n",
      "Topic 2:\n",
      " Top Terms: ['ball', 'player', 'team', 'coach', 'win']\n",
      " Weights:   ['0.500', '0.500', '0.500', '0.250', '0.250']\n",
      "\n",
      "Topic 3:\n",
      " Top Terms: ['software', 'code', 'computer', 'program', 'data']\n",
      " Weights:   ['0.447', '0.447', '0.447', '0.447', '0.447']\n"
     ]
    }
   ],
   "source": [
    "def display_topics(Vt_k, vocab, n_top_words=5):\n",
    "    print('\\n' + '='*50)\n",
    "    print('DISCOVERED LATENT TOPICS')\n",
    "    print('='*50)\n",
    "\n",
    "    for topic_idx, topic in enumerate(Vt_k):\n",
    "        # get indices of top terms (by absolute value)\n",
    "        top_indices = np.argsort(np.abs(topic))[::-1][:n_top_words]\n",
    "        top_terms = [(vocab[i], topic[i]) for i in top_indices]\n",
    "\n",
    "        print(f'\\nTopic {topic_idx + 1}:')\n",
    "        print(f' Top Terms: {[t[0] for t in top_terms]}')\n",
    "        print(f\" Weights:   {[f'{t[1]:.3f}' for t in top_terms]}\")\n",
    "\n",
    "display_topics(Vt_k, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056900c0-821d-4f7a-85fc-897ba9512c52",
   "metadata": {},
   "source": [
    "#### Step 5: Associate Topics with Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ee32d48-09f7-43db-85da-42358fb2be50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "DOCUMENT-TOPIC ASSOCIATIONS\n",
      "==================================================\n",
      "\n",
      "Doc 1 : ball game player score team\n",
      "  Topic scores: T1=0.000, T2=2.000, T3=0.000\n",
      "  → Dominant: Topic 2\n",
      "\n",
      "Doc 2 : cook recipe taste food eat\n",
      "  Topic scores: T1=2.000, T2=0.000, T3=0.000\n",
      "  → Dominant: Topic 1\n",
      "\n",
      "Doc 3 : code computer software program data\n",
      "  Topic scores: T1=0.000, T2=0.000, T3=2.236\n",
      "  → Dominant: Topic 3\n",
      "\n",
      "Doc 4 : team player win ball coach\n",
      "  Topic scores: T1=0.000, T2=2.000, T3=0.000\n",
      "  → Dominant: Topic 2\n",
      "\n",
      "Doc 5 : eat delicious cook meal taste\n",
      "  Topic scores: T1=2.000, T2=0.000, T3=0.000\n",
      "  → Dominant: Topic 1\n"
     ]
    }
   ],
   "source": [
    "# Scale U by singular values for document-topic strengths\n",
    "doc_topics = U_k * sigma_k\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DOCUMENT-TOPIC ASSOCIATIONS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for i, scores in enumerate(doc_topics):\n",
    "    dominant = np.argmax(np.abs(scores)) + 1\n",
    "    print(f\"\\nDoc {i+1} : {docs[i]}\")\n",
    "    print(f\"  Topic scores: T1={scores[0]:.3f}, T2={scores[1]:.3f}, T3={scores[2]:.3f}\")\n",
    "    print(f\"  → Dominant: Topic {dominant}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99dced9-0bda-4513-885c-d9261a4147a3",
   "metadata": {},
   "source": [
    "The last step is to give a name to each topic. This can't be done by LSA. In our toy example, it isn't difficult to label Topic 1 as 'Food', Topic 2 as 'Sports' and Topic 3 as 'Tech'. In a large dataset, this might be tasked to an LLM to perform. But an LLM is very likely to outperform this bag-of-words approach in the modelling task to begin with, something we'll explore in a later notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d664449f-67fc-4e49-9fd2-1977ce23e4b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
